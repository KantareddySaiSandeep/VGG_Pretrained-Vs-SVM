{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "#%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.applications import vgg16\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer, Activation\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "Success!\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f79abdcf2470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#trainig Data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mtrain_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mfiles_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles_0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "### Building Datasets ###\n",
    "\n",
    "files = glob.glob('train_folder/0/*')\n",
    "files_0 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/+5/*')\n",
    "files_5 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/+10/*')\n",
    "files_10 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/+15/*')\n",
    "files_15 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/+20/*')\n",
    "files_20 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/+25/*')\n",
    "files_25 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/+30/*')\n",
    "files_30 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/-5/*')\n",
    "files_n_5 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/-10/*')\n",
    "files_n_10 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/-15/*')\n",
    "files_n_15 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/-20/*')\n",
    "files_n_20 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/-25/*')\n",
    "files_n_25 = [fn for fn in files]\n",
    "files = glob.glob('train_folder/-30/*')\n",
    "files_n_30 = [fn for fn in files]\n",
    "\n",
    "\n",
    "print (len(files_0),len(files_5),len(files_10),len(files_15),len(files_20),len(files_25),len(files_30),\n",
    "      len(files_n_5),len(files_n_10),len(files_n_15),len(files_n_20),len(files_n_25),len(files_n_30))\n",
    "\n",
    "print (\"\\nSuccess!\\n\")\n",
    "\n",
    "#trainig Data\n",
    "train_0 = np.random.choice(files_0, size=200, replace=False)\n",
    "files_0 = list(set(files_0) - set(train_0))\n",
    "\n",
    "train_5 = np.random.choice(files_5, size=200, replace=False)\n",
    "files_5 = list(set(files_5) - set(train_5))\n",
    "\n",
    "train_10 = np.random.choice(files_10, size=200, replace=False)\n",
    "files_10 = list(set(files_10) - set(train_10))\n",
    "\n",
    "train_15 = np.random.choice(files_15, size=200, replace=False)\n",
    "files_15 = list(set(files_15) - set(train_15))\n",
    "\n",
    "train_20 = np.random.choice(files_20, size=200, replace=False)\n",
    "files_20 = list(set(files_20) - set(train_20))\n",
    "\n",
    "train_25 = np.random.choice(files_25, size=200, replace=False)\n",
    "files_25 = list(set(files_25) - set(train_25))\n",
    "\n",
    "train_30 = np.random.choice(files_30, size=200, replace=False)\n",
    "files_30 = list(set(files_30) - set(train_30))\n",
    "\n",
    "train_n_5 = np.random.choice(files_n_5, size=200, replace=False)\n",
    "files_n_5 = list(set(files_n_5) - set(train_n_5))\n",
    "\n",
    "train_n_10 = np.random.choice(files_n_10, size=200, replace=False)\n",
    "files_n_10 = list(set(files_n_10) - set(train_n_10))\n",
    "\n",
    "train_n_15 = np.random.choice(files_n_15, size=200, replace=False)\n",
    "files_n_15 = list(set(files_n_15) - set(train_n_15))\n",
    "\n",
    "train_n_20 = np.random.choice(files_n_20, size=200, replace=False)\n",
    "files_n_20 = list(set(files_n_20) - set(train_n_20))\n",
    "\n",
    "train_n_25 = np.random.choice(files_n_25, size=200, replace=False)\n",
    "files_n_25 = list(set(files_n_25) - set(train_n_25))\n",
    "\n",
    "train_n_30 = np.random.choice(files_n_30, size=200, replace=False)\n",
    "files_n_30 = list(set(files_n_30) - set(train_n_30))\n",
    "\n",
    "#Validation datd\n",
    "\n",
    "val_0 = np.random.choice(files_0, size=20, replace=False)\n",
    "files_0 = list(set(files_0) - set(val_0))\n",
    "\n",
    "val_5 = np.random.choice(files_5, size=20, replace=False)\n",
    "files_5 = list(set(files_5) - set(val_5))\n",
    "\n",
    "val_10 = np.random.choice(files_10, size=20, replace=False)\n",
    "files_10 = list(set(files_10) - set(val_10))\n",
    "\n",
    "val_15 = np.random.choice(files_15, size=20, replace=False)\n",
    "files_15 = list(set(files_15) - set(val_15))\n",
    "\n",
    "val_20 = np.random.choice(files_20, size=20, replace=False)\n",
    "files_20 = list(set(files_20) - set(val_20))\n",
    "\n",
    "val_25 = np.random.choice(files_25, size=20, replace=False)\n",
    "files_25 = list(set(files_25) - set(val_25))\n",
    "\n",
    "val_30 = np.random.choice(files_30, size=20, replace=False)\n",
    "files_30 = list(set(files_30) - set(val_30))\n",
    "\n",
    "val_n_5 = np.random.choice(files_n_5, size=20, replace=False)\n",
    "files_n_5 = list(set(files_n_5) - set(val_n_5))\n",
    "\n",
    "val_n_10 = np.random.choice(files_n_10, size=20, replace=False)\n",
    "files_n_10 = list(set(files_n_10) - set(val_n_10))\n",
    "\n",
    "val_n_15 = np.random.choice(files_n_15, size=20, replace=False)\n",
    "files_n_15 = list(set(files_n_15) - set(val_n_15))\n",
    "\n",
    "val_n_20 = np.random.choice(files_n_20, size=20, replace=False)\n",
    "files_n_20 = list(set(files_n_20) - set(val_n_20))\n",
    "\n",
    "val_n_25 = np.random.choice(files_n_25, size=20, replace=False)\n",
    "files_n_25 = list(set(files_n_25) - set(val_n_25))\n",
    "\n",
    "val_n_30 = np.random.choice(files_n_30, size=20, replace=False)\n",
    "files_n_30 = list(set(files_n_30) - set(val_n_30))\n",
    "\n",
    "#Test Data\n",
    "test_0 = np.random.choice(files_0, size=10, replace=False)\n",
    "files_0 = list(set(files_0) - set(test_0))\n",
    "\n",
    "test_5 = np.random.choice(files_5, size=10, replace=False)\n",
    "files_5 = list(set(files_5) - set(test_5))\n",
    "\n",
    "test_10 = np.random.choice(files_10, size=10, replace=False)\n",
    "files_10 = list(set(files_10) - set(test_10))\n",
    "\n",
    "test_15 = np.random.choice(files_15, size=10, replace=False)\n",
    "files_15 = list(set(files_15) - set(test_15))\n",
    "\n",
    "test_20 = np.random.choice(files_20, size=10, replace=False)\n",
    "files_20 = list(set(files_20) - set(test_20))\n",
    "\n",
    "test_25 = np.random.choice(files_25, size=10, replace=False)\n",
    "files_25 = list(set(files_25) - set(test_25))\n",
    "\n",
    "test_30 = np.random.choice(files_30, size=10, replace=False)\n",
    "files_30 = list(set(files_30) - set(test_30))\n",
    "\n",
    "test_n_5 = np.random.choice(files_n_5, size=10, replace=False)\n",
    "files_n_5 = list(set(files_n_5) - set(test_n_5))\n",
    "\n",
    "test_n_10 = np.random.choice(files_n_10, size=10, replace=False)\n",
    "files_n_10 = list(set(files_n_10) - set(test_n_10))\n",
    "\n",
    "test_n_15 = np.random.choice(files_n_15, size=10, replace=False)\n",
    "files_n_15 = list(set(files_n_15) - set(test_n_15))\n",
    "\n",
    "test_n_20 = np.random.choice(files_n_20, size=10, replace=False)\n",
    "files_n_20 = list(set(files_n_20) - set(test_n_20))\n",
    "\n",
    "test_n_25 = np.random.choice(files_n_25, size=10, replace=False)\n",
    "files_n_25 = list(set(files_n_25) - set(test_n_25))\n",
    "\n",
    "test_n_30 = np.random.choice(files_n_30, size=10, replace=False)\n",
    "files_n_30 = list(set(files_n_30) - set(test_n_30))\n",
    "\n",
    "\n",
    "\n",
    "print('Train Angles datasets:', train_0.shape, train_5.shape, train_10.shape, train_15.shape, train_20.shape, \n",
    "     train_25.shape, train_30.shape, train_n_5.shape, train_n_10.shape, train_n_15.shape, train_n_20.shape, \n",
    "     train_n_25.shape, train_n_30.shape)\n",
    "print('Val Angles datasets:', val_0.shape, val_5.shape, val_10.shape, val_15.shape, val_20.shape, \n",
    "     val_25.shape, val_30.shape, val_n_5.shape, val_n_10.shape, val_n_15.shape, val_n_20.shape, \n",
    "     val_n_25.shape, val_n_30.shape)\n",
    "print('Test Angles datasets:', test_0.shape, test_5.shape, test_10.shape, test_15.shape, test_20.shape, \n",
    "     test_25.shape, test_30.shape, test_n_5.shape, test_n_10.shape, test_n_15.shape, test_n_20.shape, \n",
    "     test_n_25.shape, test_n_30.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dir = 'training_data'\n",
    "val_dir = 'validation_data'\n",
    "test_dir = 'test_data'\n",
    "\n",
    "train_files = np.concatenate([train_0, train_5, train_10 , train_15, train_20, train_25, train_30, \n",
    "                              train_n_5, train_n_10, train_n_15, train_n_20, train_n_25, train_n_30])\n",
    "\n",
    "validate_files = np.concatenate([val_0, val_5, val_10 , val_15, val_20, val_25, val_30, \n",
    "                              val_n_5, val_n_10, val_n_15, val_n_20, val_n_25, val_n_30])\n",
    "\n",
    "test_files = np.concatenate([test_0, test_5, test_10 , test_15, test_20, test_25, test_30, \n",
    "                              test_n_5, test_n_10, test_n_15, test_n_20, test_n_25, test_n_30])\n",
    "\n",
    "os.mkdir(train_dir) if not os.path.isdir(train_dir) else None\n",
    "os.mkdir(val_dir) if not os.path.isdir(val_dir) else None\n",
    "os.mkdir(test_dir) if not os.path.isdir(test_dir) else None\n",
    "\n",
    "for fn in train_files:\n",
    "   shutil.copy(fn, train_dir)\n",
    "\n",
    "for fn in validate_files:\n",
    "    shutil.copy(fn, val_dir)\n",
    "    \n",
    "for fn in test_files:\n",
    "    shutil.copy(fn, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (2600, 150, 150, 3) \tValidation dataset shape: (260, 150, 150, 3)\n",
      "(150, 150, 3)\n",
      "(2600, 13) (260, 13)\n"
     ]
    }
   ],
   "source": [
    "IMG_DIM = (150, 150)\n",
    "\n",
    "train_files = glob.glob('training_data/*')\n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_labels = [fn.split('\\\\')[1].split('.')[0].strip() for fn in train_files]\n",
    "\n",
    "validation_files = glob.glob('validation_data/*')\n",
    "validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]\n",
    "validation_imgs = np.array(validation_imgs)\n",
    "validation_labels = [fn.split('\\\\')[1].split('.')[0].strip() for fn in validation_files]\n",
    "\n",
    "print('Train dataset shape:', train_imgs.shape, \n",
    "      '\\tValidation dataset shape:', validation_imgs.shape)\n",
    "\n",
    "\n",
    "train_imgs_scaled = train_imgs.astype('float32')\n",
    "validation_imgs_scaled  = validation_imgs.astype('float32')\n",
    "train_imgs_scaled /= 255\n",
    "validation_imgs_scaled /= 255\n",
    "\n",
    "print(train_imgs[0].shape)\n",
    "array_to_img(train_imgs[0])\n",
    "\n",
    "train_labels = [int(y) for y in [x.replace('0', '0').replace('+5', '1').replace('+10', '2').replace('+15', '3').replace('+20', '4').replace('+25', '5').replace('+30', '6').replace('-5', '7').replace('-10', '8').replace('-15', '9').replace('-20', '10').replace('-25', '11').replace('-30', '12') for x in train_labels]]\n",
    "validation_labels = [int(y) for y in [x.replace('0', '0').replace('+5', '1').replace('+10', '2').replace('+15', '3').replace('+20', '4').replace('+25', '5').replace('+30', '6').replace('-5', '7').replace('-10', '8').replace('-15', '9').replace('-20', '10').replace('-25', '11').replace('-30', '12') for x in validation_labels]]\n",
    "\n",
    "\n",
    "batch_size = 60\n",
    "num_classes = 13\n",
    "epochs = 30\n",
    "input_shape = (150, 150, 3)\n",
    "\n",
    "# encode text category labels\n",
    "train_labels_enc = np_utils.to_categorical(train_labels)\n",
    "validation_labels_enc = np_utils.to_categorical(validation_labels)\n",
    "\n",
    "print(train_labels_enc.shape, validation_labels_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 512)\n"
     ]
    }
   ],
   "source": [
    "### Pre-trained CNN model as a Feature Extractor ###\n",
    "vgg = vgg16.VGG16(include_top=False, weights='imagenet', \n",
    "                                     input_shape=input_shape)\n",
    "\n",
    "output = vgg.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "vgg_model = Model(vgg.input, output)\n",
    "\n",
    "vgg_model.trainable = False\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "pd.set_option('max_colwidth', -1)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
    "\n",
    "\n",
    "bottleneck_feature_example = vgg.predict(train_imgs_scaled[0:1])\n",
    "print(bottleneck_feature_example.shape)\n",
    "plt.imshow(bottleneck_feature_example[0][:,:,0])\n",
    "\n",
    "def get_bottleneck_features(model, input_imgs):\n",
    "    features = model.predict(input_imgs, verbose=0)\n",
    "    return features\n",
    "    \n",
    "train_features_vgg = get_bottleneck_features(vgg_model, train_imgs_scaled)\n",
    "validation_features_vgg = get_bottleneck_features(vgg_model, validation_imgs_scaled)\n",
    "\n",
    "print('Train Bottleneck Features:', train_features_vgg.shape, \n",
    "      '\\tValidation Bottleneck Features:', validation_features_vgg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = vgg_model.output_shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(input_shape,)))\n",
    "# model.add(vgg_model)\n",
    "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Flatten())\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=train_features_vgg, y=train_labels_enc,\n",
    "                    validation_data=(validation_features_vgg, validation_labels_enc),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    shuffle = True,\n",
    "                    verbose=1)\n",
    "\n",
    "model.save('cheque_rotation_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Last Layer and Adding SVM on top of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\v-parv.garg\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\v-parv.garg\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\v-parv.garg\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 13)                6669      \n",
      "=================================================================\n",
      "Total params: 4,464,141\n",
      "Trainable params: 4,464,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 4,464,141\n",
      "Trainable params: 4,464,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\v-parv.garg\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "basic_cnn = load_model('cheque_rotation_1.h5')\n",
    "model =basic_cnn\n",
    "model.summary()\n",
    "model._layers.pop()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\v-parv.garg\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(13, activation=\"softmax\", kernel_regularizer=<keras.reg...)`\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_features_vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-516ad49ded69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adadelta'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m---> 11\u001b[1;33m model.fit(x=train_features_vgg, y=train_labels_enc,\n\u001b[0m\u001b[0;32m     12\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_features_vgg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_labels_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features_vgg' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(13, W_regularizer=l2(0.01),activation='softmax'))\n",
    "model.compile(loss='squared_hinge',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=train_features_vgg, y=train_labels_enc,\n",
    "                    validation_data=(validation_features_vgg, validation_labels_enc),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    shuffle = True,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
